---
title: "ECON7333: Assignment 3"
author: "Thomas Doyle"
output:
  pdf_document: default
  html_document: 
    keep_md: yes
editor_options:
  chunk_output_type: inline
---

#### Setup

```{r}
options(scipen=999)
attach(ISLR::Wage)
```

# Exercise 1

## Exercise 1.1

The following function, `loocv_tmse`, computes the test mean square error of a regression model using a leave-one-out cross-validation approach.
```{r}

loocv_tmse <- function(d){
  #' @d a data.frame returned by `model.frame()`
  #
  n <- dim(d)[1]
  p <- dim(d)[2]
  
  MSE <- rep(0,n)
  
  for(i in 1:n) {
    #
    lm_i <- lm(d[-i,],y=TRUE) # leave i out
    MSE_i <- (lm_i$y[i]-predict(lm_i,d[i,]))^2
    MSE[i] <- MSE_i
  }
  
  return(
    # 
    CV_n <- mean(MSE,na.rm = TRUE)
  )
}

```

## Exercise 1.2

We use `loocv_tmse` to choose beteen the three models;

1. $logwage=\beta_0+\beta_1 age$
2. $logwage=\beta_0+\beta_1\;age + \beta_2\; age^2$
3. $logwage=\beta_0+\beta_1\;age + \beta_2\; education$

Each model takes `logwage` as output variable.
```{r}
Wage.models <- list(
  model.frame("logwage~age",ISLR::Wage),
  model.frame("logwage~age+I(age^2)",ISLR::Wage),
  model.frame("logwage~age+education",ISLR::Wage)
)
sapply(Wage.models, loocv_tmse)
```

## Exercise 1.3

The $\lambda$ parameter is now used as tuning parameter on the ridge regression model;
$$logwage=\beta_0+\beta_1\;age + \beta_2\; education$$

```{r}

lm.ridge <- function(lambda=0) {
  
  m <- model.frame(logwage~age+education,ISLR::Wage)
  Terms <- attr(eval.parent(m),"terms")
  Y <- model.response(m)
  X <- model.matrix(object = Terms,data = m,contrasts.arg = list(education="contr.treatment"))
  n <- nrow(X)
  p <- ncol(X)
  
  # https://arxiv.org/pdf/1509.09169.pdf
  # Hoerl, A. E. and Kennard, R. W. (1970). 
  ridge.betas <- solve( t(X) %*% X + lambda*diag(p) ) %*% t(X) %*% Y
  
  ## predict Y
  # resid <- (Y-X%*%ridge.betas) 
  # penalty <- lambda*sum(ridge.betas[-1]^2)
  # rss <- sum(resid^2)
  
  l2.norm <- sqrt(sum(ridge.betas[-1]^2)) # drop intercept
  
  return(
    list(
      log.lambda=log(lambda),
      l2.norm=l2.norm,
      coefficients=ridge.betas,
      lambda=lambda
    )
  )
}

```

```{r}
# MLE
# showing off
lm.ridge.loocv <- function(limits) {
  #' @limits 
  #

  m <- model.frame(logwage~age+education,ISLR::Wage)
  Terms <- attr(eval.parent(m),"terms")
  Y <- model.response(m)
  X <- model.matrix(object = Terms,data = m,contrasts.arg = list(education="contr.treatment"))
  
  loocv <- function(lambda, X, Y, Delta){
    n <- nrow(X)
    p <- ncol(X)
    loss <- 0
    
    for (i in 1:n) {
      loo_beta <- solve(t(X[-i,])%*%X[-i,]+lambda*diag(p))%*%t(X[-i,])%*%Y[-i]
      loss <- loss+(Y[i]-X[i,1]*loo_beta[1]-X[i,-1]%*%loo_beta[-1])^2
    }
    
    return(loss)
  }
  
  # optimize penalty parameter
  # minimize RSS
  opt <- optimize(loocv, limits, X=X, Y=Y)
  l2.norm <- lm.ridge(opt$minimum)$l2.norm
  
  return(
    data.frame(opt,l2.norm)
  ) 
}

```

Plot $\ln(\lambda)$ against $\ell_2\;\text{norm}$.

```{r}
lambdas <- 100^seq(-2, 5, length = 60)
plot(t(sapply(lambdas,lm.ridge))[,1:2],type="l")

# mle
lm.ridge.loocv(c(10^-10, 10^10))
```


# Exercise 2

## Exercise 2.1

```{r}
dgf <- function(n,p) {
  #' @n `scalar`
  #' @p `scalar`
  
  e <- rnorm(n,0,1)
  X <- matrix(runif(n*p),n,p)
  y <- 2*X[,1]+4*X[,2]+e

  output <- list(y=y,X=X)
  return(list(y,X))
}

p.5 <- dgf(100,5)
y <- p.5[[1]]
X <- p.5[[2]]

```

## Exercise 2.2

```{r}

model_path <- function(y,X,M) {
  #' @y `vector`, response variable
  #' @X `matrix`, input variable
  #' @M `scalar`, model path
  
  p <- dim(X)[2]
  n <- dim(X)[1]
  
  # Initialise
  # 
  b <- matrix(rep(NA,p*M),p,M)
  b[,1] <- 0
  r <- as.vector(y)
  # e <- as.vector(rep(0.01,n))
  e <- 0.01
  
  for(m in 2:M) {
    z <- b[,m-1]
    
    ## select x_j with highest correlation with r
    j <- which.max(cor(x=X,y=r))
    xj <- X[,j]
    
    a <- suppressWarnings(e*sign(t(xj)%*%r))
    z[j] <- z[j]+a
    b[,m] <- z
    r <- as.vector(r-a%*%xj)
  }
  
  return(b)
}

```

## Exercise 2.3

Generate a sample path of $M=1000$ coeficient estimates   $\hat{\beta}_j^{(m)}$, and plot the sample using `matplot`.

```{r}

path <- model_path(y,X,1000)

par(mfrow=c(1,2))
matplot(path,pch=2, cex=.2, col="black",
        ylab="level estimates",
        xlab=expression(hat(beta)[j]^(m)),
        main="")

plot(lm(y~X-1)$coefficients, pch=20, cex=.8,
     ylab="level coeficient estimates",
     xlab=expression(hat(beta)),
     main="y~X")
```

## Exercise 2.4

```{r}
mse_i <- function(y,X,m,i) {
  mean(sum((y[i] - m%*%t(X[i,]))^2))
}

subset_selection <- function(y,X,M,k) {
  #' @y
  #' @X
  #' @M
  #' @k 
  
  # calculate MSE by iterate over M
  MSE_i <- apply(M,2,function(m) {
    mse_i(y,X,m,k)
  })
  
  # Return coef for model with min MSE
  return(
    list(
      coefficients = M[,which.min(MSE_i)],
      best_subset_m = which.min(MSE_i)
    )
  )
}

k_fold <- function(n,k) {
  
  ix <- sample(1:n,n)
  
  fold_i <- split(ix,cut(1:n,k,FALSE))
  
  folds <- data.frame(fold_i)
  
  colnames(folds) <- paste("fold",1:k,sep = "_")
  
  return(folds)
}

```

```{r}

folds <- k_fold(100,2)

# obtain test MSE using k-fold
fold_1_tmse <- mse_i(y,X,subset_selection(y,X,path,folds[,1])[["coefficients"]],folds[,2])
fold_2_tmse <- mse_i(y,X,subset_selection(y,X,path,folds[,2])[["coefficients"]],folds[,1])
which.min(c(fold_1_tmse,fold_2_tmse))

subset_selection(y,X,path,folds[,1])
subset_selection(y,X,path,folds[,2])

LM_MSE <- sum(lm(y~X-1)$residuals^2)
```

```{r}
CV_k <- function(n,p) {
  
  # call dgf to generate new n x p data
  d <- dgf(n,p)
  
  y <- d[[1]]
  X <- d[[2]]
  
  # generate models
  path <- model_path(y,X,1000)
  
  folds <- k_fold(n,2)

  # obtain test MSE using k-fold
  tmse_1 <- mse_i(y,X,subset_selection(y,X,path,folds[,1])[["coefficients"]],folds[,2])
  tmse_1 <- mse_i(y,X,train(y,X,path,folds[,2])[["coefficients"]],folds[,1])
  LM_MSE <- sum(lm(y~X-1)$residuals^2)
  return(list(MSE_1,MSE_2,LM_MSE))
}


CV_k(100,5)
CV_k(100,10)
CV_k(100,20)
CV_k(100,50)
```
